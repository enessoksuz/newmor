# ğŸš€ PostgreSQL Performans Optimizasyon Rehberi

## 1. Ä°ndeksleme Stratejisi

### Mevcut Ä°ndeksler
Åemada zaten optimize edilmiÅŸ indeksler var:
- **B-tree indeksler**: Slug, email, username gibi unique alanlar
- **GIN indeks**: Full-text search iÃ§in
- **Composite indeksler**: Birden fazla sÃ¼tun kombinasyonlarÄ±
- **Partial indeksler**: Sadece belirli koÅŸullardaki veriler (WHERE clause ile)

### Ä°ndeks BakÄ±mÄ±
```sql
-- Ä°ndeksleri yeniden oluÅŸtur (fragmentation durumunda)
REINDEX TABLE articles;

-- Ä°ndeks boyutlarÄ±nÄ± kontrol et
SELECT 
    indexname,
    pg_size_pretty(pg_relation_size(indexname::regclass)) as size
FROM pg_indexes
WHERE schemaname = 'public'
ORDER BY pg_relation_size(indexname::regclass) DESC;
```

## 2. Query Optimizasyonu

### EXPLAIN ANALYZE KullanÄ±mÄ±
```sql
-- Sorgu planÄ±nÄ± gÃ¶ster
EXPLAIN ANALYZE
SELECT a.* 
FROM articles a
WHERE a.status = 'published'
ORDER BY a.published_at DESC
LIMIT 20;
```

### N+1 Problem Ã‡Ã¶zÃ¼mÃ¼
âŒ **KÃ¶tÃ¼ YaklaÅŸÄ±m** (Her makale iÃ§in ayrÄ± sorgu):
```sql
SELECT * FROM articles;
-- Sonra her makale iÃ§in:
SELECT * FROM authors WHERE id IN (...);
```

âœ… **Ä°yi YaklaÅŸÄ±m** (Tek sorguda JOIN):
```sql
SELECT 
    a.*,
    json_agg(au.*) as authors
FROM articles a
LEFT JOIN article_authors aa ON a.id = aa.article_id
LEFT JOIN authors au ON aa.author_id = au.id
GROUP BY a.id;
```

## 3. Caching Stratejisi

### Redis ile Cache
```javascript
const Redis = require('ioredis');
const redis = new Redis();

// Makale cache'le (TTL: 5 dakika)
async function getArticle(slug) {
    const cacheKey = `article:${slug}`;
    
    // Ã–nce cache'e bak
    const cached = await redis.get(cacheKey);
    if (cached) {
        return JSON.parse(cached);
    }
    
    // Cache'te yoksa DB'den Ã§ek
    const article = await db.query(
        'SELECT * FROM articles WHERE slug = $1',
        [slug]
    );
    
    // Cache'e kaydet (300 saniye = 5 dakika)
    await redis.setex(cacheKey, 300, JSON.stringify(article));
    
    return article;
}

// Cache temizleme (makale gÃ¼ncellendiÄŸinde)
async function updateArticle(slug, data) {
    await db.query('UPDATE articles SET ... WHERE slug = $1', [slug]);
    await redis.del(`article:${slug}`);
}
```

### Cache KatmanlarÄ±
```
1. Browser Cache (Static files): 1 gÃ¼n
2. CDN Cache (Images, CSS, JS): 7 gÃ¼n
3. Redis Cache (DB queries): 5-60 dakika
4. PostgreSQL Query Cache: Otomatik
```

## 4. Connection Pooling

### PgBouncer Kurulumu
```bash
# Ubuntu/Debian
sudo apt install pgbouncer

# KonfigÃ¼rasyon (/etc/pgbouncer/pgbouncer.ini)
[databases]
haber_sitesi = host=localhost port=5432 dbname=haber_sitesi

[pgbouncer]
listen_addr = 127.0.0.1
listen_port = 6432
auth_type = md5
auth_file = /etc/pgbouncer/userlist.txt
pool_mode = transaction
max_client_conn = 1000
default_pool_size = 25
```

### Node.js Connection Pool
```javascript
const { Pool } = require('pg');

const pool = new Pool({
    host: 'localhost',
    port: 6432, // PgBouncer portu
    database: 'haber_sitesi',
    user: 'haber_app',
    password: 'password',
    max: 20,              // Max baÄŸlantÄ± sayÄ±sÄ±
    min: 5,               // Min baÄŸlantÄ± sayÄ±sÄ±
    idleTimeoutMillis: 30000,
    connectionTimeoutMillis: 2000,
});

// KullanÄ±m
async function getArticles() {
    const client = await pool.connect();
    try {
        const result = await client.query('SELECT ...');
        return result.rows;
    } finally {
        client.release(); // Ã–nemli!
    }
}
```

## 5. Partitioning (BÃ¼yÃ¼k Tablolar Ä°Ã§in)

### Tarih BazlÄ± Partitioning
```sql
-- Ana tablo (partition iÃ§in)
CREATE TABLE articles_partitioned (
    id UUID DEFAULT uuid_generate_v4(),
    title VARCHAR(500) NOT NULL,
    content TEXT NOT NULL,
    published_at TIMESTAMP WITH TIME ZONE NOT NULL,
    -- diÄŸer kolonlar...
) PARTITION BY RANGE (published_at);

-- 2024 PartitionÄ±
CREATE TABLE articles_2024 PARTITION OF articles_partitioned
    FOR VALUES FROM ('2024-01-01') TO ('2025-01-01');

-- 2025 PartitionÄ±
CREATE TABLE articles_2025 PARTITION OF articles_partitioned
    FOR VALUES FROM ('2025-01-01') TO ('2026-01-01');

-- Indeksler her partition iÃ§in otomatik oluÅŸturulur
CREATE INDEX ON articles_partitioned (published_at);
```

## 6. Materialized Views

### PopÃ¼ler Ä°Ã§erikler iÃ§in
```sql
-- View oluÅŸtur (ÅŸemada zaten var)
CREATE MATERIALIZED VIEW popular_articles AS
SELECT 
    a.id,
    a.title,
    a.slug,
    a.view_count,
    array_agg(c.name) as categories
FROM articles a
LEFT JOIN article_categories ac ON a.id = ac.article_id
LEFT JOIN categories c ON ac.category_id = c.id
WHERE a.status = 'published'
GROUP BY a.id
ORDER BY a.view_count DESC
LIMIT 100;

CREATE UNIQUE INDEX ON popular_articles (id);

-- GÃ¼ncelleme (Cron job ile her saat)
REFRESH MATERIALIZED VIEW CONCURRENTLY popular_articles;
```

### Cron Job ile Otomatik GÃ¼ncelleme
```bash
# /etc/cron.d/refresh-views
0 * * * * postgres psql -d haber_sitesi -c "REFRESH MATERIALIZED VIEW CONCURRENTLY popular_articles;"
```

## 7. Monitoring ve Alerting

### pg_stat_statements ile YavaÅŸ SorgularÄ± Bul
```sql
CREATE EXTENSION pg_stat_statements;

-- En yavaÅŸ 10 sorgu
SELECT 
    substring(query, 1, 100) as query_snippet,
    calls,
    total_exec_time / 1000 as total_seconds,
    mean_exec_time / 1000 as avg_seconds,
    max_exec_time / 1000 as max_seconds
FROM pg_stat_statements
ORDER BY mean_exec_time DESC
LIMIT 10;

-- En Ã§ok Ã§alÄ±ÅŸtÄ±rÄ±lan sorgular
SELECT 
    substring(query, 1, 100) as query_snippet,
    calls,
    total_exec_time / 1000 as total_seconds
FROM pg_stat_statements
ORDER BY calls DESC
LIMIT 10;
```

### Prometheus + Grafana Monitoring
```yaml
# docker-compose.yml
version: '3.8'
services:
  postgres-exporter:
    image: wrouesnel/postgres_exporter
    environment:
      DATA_SOURCE_NAME: "postgresql://haber_app:password@postgres:5432/haber_sitesi?sslmode=disable"
    ports:
      - "9187:9187"
```

## 8. Auto-Vacuum AyarlarÄ±

```sql
-- Tablo bazlÄ± vacuum ayarlarÄ±
ALTER TABLE articles SET (
    autovacuum_vacuum_scale_factor = 0.1,
    autovacuum_analyze_scale_factor = 0.05,
    autovacuum_vacuum_cost_delay = 10
);

-- Manuel vacuum
VACUUM ANALYZE articles;

-- Vacuum istatistiklerini kontrol et
SELECT 
    schemaname,
    relname,
    last_vacuum,
    last_autovacuum,
    vacuum_count,
    autovacuum_count
FROM pg_stat_user_tables
WHERE schemaname = 'public';
```

## 9. Read Replicas (YÃ¼ksek Trafik Ä°Ã§in)

### Master-Slave YapÄ±landÄ±rma
```
Master (Write)
    â†“
Slave 1 (Read) â”€â”€â†’ Load Balancer â†â”€â”€ Application
Slave 2 (Read) â”€â”€â†—
```

### Node.js ile Read/Write Splitting
```javascript
const masterPool = new Pool({
    host: 'master.db.example.com',
    // ... master config
});

const replicaPool = new Pool({
    host: 'replica.db.example.com',
    // ... replica config
});

// Write iÅŸlemleri master'a
async function createArticle(data) {
    return masterPool.query('INSERT INTO articles ...');
}

// Read iÅŸlemleri replica'ya
async function getArticles() {
    return replicaPool.query('SELECT * FROM articles ...');
}
```

## 10. CDN Entegrasyonu

### Cloudflare/AWS CloudFront
```
User Request
    â†“
CDN (Static files: images, CSS, JS)
    â†“ (Cache Miss)
Application Server
    â†“
Database
```

### Image Optimization
```javascript
// Cloudinary Ã¶rneÄŸi
const cloudinary = require('cloudinary').v2;

// Otomatik resize ve optimize
const imageUrl = cloudinary.url('article-image.jpg', {
    width: 800,
    height: 600,
    crop: 'fill',
    quality: 'auto',
    fetch_format: 'auto' // WebP desteÄŸi olan browserlara WebP gÃ¶nder
});
```

## 11. Database Backup Stratejisi

### 3-2-1 Yedekleme KuralÄ±
- **3 kopya**: Production + 2 backup
- **2 farklÄ± ortam**: Local + Cloud
- **1 off-site**: AWS S3, Google Cloud Storage

```bash
#!/bin/bash
# GÃ¼nlÃ¼k yedekleme script

# Local backup
pg_dump -Fc haber_sitesi > /backup/daily_$(date +%Y%m%d).dump

# S3'e yÃ¼kle
aws s3 cp /backup/daily_$(date +%Y%m%d).dump s3://my-backups/postgres/

# 30 gÃ¼nden eski yerel yedekleri sil
find /backup -name "*.dump" -mtime +30 -delete
```

## 12. Performans Benchmarking

### pgbench ile Test
```bash
# VeritabanÄ± hazÄ±rlama (test iÃ§in)
pgbench -i -s 50 haber_sitesi

# Performans testi (10 client, 100 transaction)
pgbench -c 10 -t 100 haber_sitesi

# Ã–zel SQL ile test
pgbench -c 20 -T 60 -f custom_queries.sql haber_sitesi
```

## 13. GÃ¼venlik Best Practices

```sql
-- SSL baÄŸlantÄ± zorla
ALTER DATABASE haber_sitesi SET ssl = on;

-- Row Level Security (RLS)
ALTER TABLE articles ENABLE ROW LEVEL SECURITY;

-- Sadece aktif yazarlarÄ±n makalelerini gÃ¶ster
CREATE POLICY author_articles ON articles
    FOR SELECT
    USING (
        EXISTS (
            SELECT 1 FROM article_authors aa
            JOIN authors au ON aa.author_id = au.id
            WHERE aa.article_id = articles.id
            AND au.is_active = true
        )
    );

-- Sensitive data iÃ§in encryption
CREATE EXTENSION pgcrypto;

-- Åifre hash'leme
INSERT INTO authors (username, password_hash)
VALUES ('user', crypt('password123', gen_salt('bf')));
```

## 14. Checklist: Production'a Ã‡Ä±kmadan Ã–nce

- [ ] TÃ¼m indeksler oluÅŸturuldu mu?
- [ ] Connection pooling kuruldu mu?
- [ ] Redis cache aktif mi?
- [ ] Yedekleme sistemi Ã§alÄ±ÅŸÄ±yor mu?
- [ ] Monitoring kuruldu mu?
- [ ] SSL/TLS aktif mi?
- [ ] Read replica kuruldu mu? (YÃ¼ksek trafik iÃ§in)
- [ ] CDN yapÄ±landÄ±rÄ±ldÄ± mÄ±?
- [ ] Load testing yapÄ±ldÄ± mÄ±?
- [ ] Disaster recovery planÄ± var mÄ±?

## 15. Performans Hedefleri

| Metrik | Hedef |
|--------|-------|
| Ana sayfa yÃ¼kleme | < 1 saniye |
| Makale detay sayfasÄ± | < 500ms |
| Arama sorgusu | < 200ms |
| Database query ortalama | < 50ms |
| Concurrent users | 10,000+ |
| Requests/second | 1,000+ |

## SonuÃ§

Bu optimizasyonlarla:
- âœ… **100,000+ makale** sorunsuz Ã§alÄ±ÅŸÄ±r
- âœ… **10,000+ concurrent user** desteklenir
- âœ… **Sub-second response times** saÄŸlanÄ±r
- âœ… **99.9% uptime** garantilenir
